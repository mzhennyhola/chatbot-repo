{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**What is a chatbot**?<br>\n",
        "A ChatBot is a kind of virtual assistant that can build conversations with human users! A Chatting Robot. \n",
        "Building a chatbot is one of the popular tasks in Natural Language Processing.\n",
        "\n",
        "Are all chatbots the same?\n",
        "Chatbots fall under three common categories:\n",
        "\n",
        "1. ***Rule-based chatbots***: These bots respond to users inputs based on certain pre-specified rules. \n",
        "    For instance, these rules can be defined as if-elif-else statements. \n",
        "    While writing rules for these chatbots, it is important to expect all possible user inputs, else the bot may fail to answer properly. \n",
        "    Hence, rule-based chatbots do not possess any cognitive skills.\n",
        "\n",
        "2. ***Retrieval-based chatbots***: These bots respond to users inputs by retrieving the most relevant information from the given text document. \n",
        "    The most relevant information can be determined by Natural Language Processing with a scoring system such as cosine-similarity-score. \n",
        "    Though these bots use NLP to do conversations, they lack cognitive skills to match a real human chatting companion.\n",
        "\n",
        "3. ***Intelligent chatbots***: These bots respond to users' inputs after understanding the inputs, as humans do. \n",
        "    These bots are trained with a Machine Learning Model on a large training dataset of human conversations. \n",
        "    These bots are cognitive to match a human in conversing. Amazon's Alexa, Apple's Siri fall under this category. \n",
        "    Further, most of these bots can make conversations based on the preceding chat texts."
      ],
      "metadata": {
        "id": "ZSXpxwJjROPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_Ns1tHdAO1W",
        "outputId": "821d3a7a-9b86-454b-8969-c022bdb7d11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import nltk\n",
        "import nltk\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in dataset\n",
        "with open('/content/dialogs.txt', 'r', encoding='utf8', errors='ignore') as file:\n",
        "    dataset = file.read()\n",
        "\n",
        "# Here, the Next step is tokenize our text dataset.\n",
        "# There are two types of tokenization:\n",
        "    # 1. Word Tokenization: This is  the process of breaking down a text or document into individual words or tokens.\n",
        "    # 2. Sent Tokenization: This is to break down the text data into individual sentences so that each sentence can be processed separately.\n",
        "# Lemmatization: The goal of lemmatization is to reduce a word to its canonical form so that variations of the same word can be treated as the same token\n",
        "# For example, the word \"jumped\" may be lemmatized to \"jump\", and the word \"walking\" may be lemmatized to \"walk\".\n",
        "# By reducing words to their base forms, lemmatization can help to simplify text data and reduce the number of unique tokens that need to be analyzed or processed.\n",
        "\n",
        "# We Therefore Preprocess our text dataset by TOKENIZING and Lemmatizing them\n",
        "sent_tokens = nltk.sent_tokenize(dataset)\n",
        "word_tokens = nltk.word_tokenize(dataset)\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(tokens):\n",
        "    return [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalnum()]\n",
        "# The code above does the following:\n",
        "    # 1. Turns the word to a lower case (.lower())\n",
        "    # 2. Checks if it's all alphanumeric (.isalnum())\n",
        "    # 3. Lemmatizes the word if the word is turned is alphanumeric and lowercase.\n",
        "\n",
        "\n",
        "corpus = [\" \".join(preprocess(nltk.word_tokenize(sentence))) for sentence in sent_tokens]\n",
        "# The code above does the following:\n",
        "    # 1. Runs the preprocess function created above on the sent_tokens list we created before.\n",
        "    # 2. Then joins all this words with a space\n",
        "\n",
        "\n",
        "# Vectorize corpus\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "# TDIDF is a numerical statistic used to evaluate how important a word is to a document in a collection or corpus. \n",
        "# The TfidfVectorizer calculates the Tfidf values for each word in the corpus and uses them to create a matrix where each row represents a document and each column represents a word. \n",
        "# The cell values in the matrix correspond to the importance of each word in each document.\n",
        "\n",
        "\n",
        "# Define chatbot function\n",
        "def chatbot_response(user_input):\n",
        "    # Preprocess user input\n",
        "    user_input = \" \".join(preprocess(nltk.word_tokenize(user_input)))\n",
        "\n",
        "    # Vectorize user input\n",
        "    user_vector = vectorizer.transform([user_input])\n",
        "\n",
        "    # Calculate cosine similarity between user input and corpus\n",
        "    similarities = cosine_similarity(user_vector, X)\n",
        "\n",
        "    # Get index of most similar sentence\n",
        "    idx = similarities.argmax()\n",
        "\n",
        "    # Return corresponding sentence from corpus\n",
        "    return sent_tokens[idx]\n",
        "\n",
        "# Run chatbot\n",
        "print(\"Welcome to the chatbot! How can I help you today?\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"> \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    response = chatbot_response(user_input)\n",
        "    print(response)\n"
      ],
      "metadata": {
        "id": "3f4jMMFxFqpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We begin by importing the necessary libraries, including nltk, random, sklearn.feature_extraction.text, and sklearn.metrics.pairwise.\n",
        "\n",
        "# We download the necessary NLTK data using the nltk.download() function.\n",
        "\n",
        "# We read in the dataset from a text file using the open() function.\n",
        "\n",
        "# We preprocess the dataset using the nltk.sent_tokenize(), nltk.word_tokenize(), and nltk.stem.WordNetLemmatizer() functions.\n",
        "\n",
        "# We vectorize the preprocessed dataset using the sklearn.feature_extraction.text.TfidfVectorizer() function.\n",
        "\n",
        "# We define the chatbot_response() function, which takes a user input, preprocesses it, vectorizes it, calculates the cosine similarity between the user input and the corpus, and returns the most similar sentence from the corpus.\n",
        "\n",
        "# We run the chatbot by prompting the user for input using the input() function, passing the user input to the chatbot_response() function, and printing the resulting response.\n",
        "\n",
        "# We allow the user to quit the chatbot by entering the word \"quit\"."
      ],
      "metadata": {
        "id": "X-h1DtUsG1oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><hr><hr><br>\n",
        "<h1><b>TRANSFER LEARNING CHATBOT WITH MICROSOFT DIALOG GPT</h1>"
      ],
      "metadata": {
        "id": "K2GYvnnUOzaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --q\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n"
      ],
      "metadata": {
        "id": "x1Q20mshShgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "c6bCdqfdS70s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBot():\n",
        "    # initialize\n",
        "    def __init__(self):\n",
        "        # once chat starts, the history will be stored for chat continuity\n",
        "        self.chat_history_ids = None\n",
        "        # make input ids global to use them anywhere within the object\n",
        "        self.bot_input_ids = None\n",
        "        # a flag to check whether to end the conversation\n",
        "        self.end_chat = False\n",
        "        # greet while starting\n",
        "        self.welcome()\n",
        "        \n",
        "    def welcome(self):\n",
        "        print(\"Initializing ChatBot ...\")\n",
        "        print('Type \"bye\" or \"quit\" or \"exit\" to end chat \\n')\n",
        "\n",
        "        # Greet and introduce\n",
        "        greeting = np.random.choice([\n",
        "            \"Welcome, I am ChatBot, here for your kind service\",\n",
        "            \"Hey, Great day! I am your virtual assistant\",\n",
        "            \"Hello, it's my pleasure meeting you\",\n",
        "            \"Hi, I am a ChatBot. Let's chat!\"\n",
        "        ])\n",
        "        print(\"ChatBot >>  \" + greeting)\n",
        "        \n",
        "    def user_input(self):\n",
        "        # receive input from user\n",
        "        text = input(\"User    >> \")\n",
        "        # end conversation if user wishes so\n",
        "        if text.lower().strip() in ['bye', 'quit', 'exit']:\n",
        "            # turn flag on \n",
        "            self.end_chat=True\n",
        "            # a closing comment\n",
        "            print('ChatBot >>  See you soon! Bye!')\n",
        "            # time.sleep(1)\n",
        "            print('\\nQuitting ChatBot ...')\n",
        "        else:\n",
        "            # continue chat, preprocess input text\n",
        "            # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "            self.new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, \\\n",
        "                                                       return_tensors='pt')\n",
        "\n",
        "    def bot_response(self):\n",
        "        # append the new user input tokens to the chat history\n",
        "        # if chat has already begun\n",
        "        if self.chat_history_ids is not None:\n",
        "            self.bot_input_ids = torch.cat([self.chat_history_ids, self.new_user_input_ids], dim=-1) \n",
        "        else:\n",
        "            # if first entry, initialize bot_input_ids\n",
        "            self.bot_input_ids = self.new_user_input_ids\n",
        "        \n",
        "        # define the new chat_history_ids based on the preceding chats\n",
        "        # generated a response while limiting the total chat history to 1000 tokens, \n",
        "        self.chat_history_ids = model.generate(self.bot_input_ids, max_length=1000, \\\n",
        "                                               pad_token_id=tokenizer.eos_token_id)\n",
        "            \n",
        "        # last ouput tokens from bot\n",
        "        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[-1]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # in case, bot fails to answer\n",
        "        if response == \"\":\n",
        "            response = self.random_response()\n",
        "        # print bot response\n",
        "        print('ChatBot >>  '+ response)\n",
        "        \n",
        "    # in case there is no response from model\n",
        "    def random_response(self):\n",
        "        i = -1\n",
        "        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # iterate over history backwards to find the last token\n",
        "        while response == '':\n",
        "            i = i-1\n",
        "            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # if it is a question, answer suitably\n",
        "        if response.strip() == '?':\n",
        "            reply = np.random.choice([\"I don't know\", \n",
        "                                     \"I am not sure\"])\n",
        "        # not a question? answer suitably\n",
        "        else:\n",
        "            reply = np.random.choice([\"Great\", \n",
        "                                      \"Fine. What's up?\", \n",
        "                                      \"Okay\"\n",
        "                                     ])\n",
        "        return reply"
      ],
      "metadata": {
        "id": "QZfkC_LfjkJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a ChatBot object\n",
        "bot = ChatBot()\n",
        "# start chatting\n",
        "while True:\n",
        "    # receive user input\n",
        "    bot.user_input()\n",
        "    # check whether to end chat\n",
        "    if bot.end_chat:\n",
        "        break\n",
        "    # output bot response\n",
        "    bot.bot_response()   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxthKEDWNPEb",
        "outputId": "5712f192-cacc-42f4-fa45-726ef638497e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing ChatBot ...\n",
            "Type \"bye\" or \"quit\" or \"exit\" to end chat \n",
            "\n",
            "ChatBot >>  Welcome, I am ChatBot, here for your kind service\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatBot >>  Hey, you.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatBot >>  I know weeks.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatBot >>  It's sunday over here.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatBot >>  I know that. I was just making a joke.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatBot >>  I don't know.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatBot >>  I'm a little confused.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatBot >>  I am your creator.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatBot >>  I am your creator.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatBot >>  I am your creator.\n",
            "User    >> quit\n",
            "ChatBot >>  See you soon! Bye!\n",
            "\n",
            "Quitting ChatBot ...\n"
          ]
        }
      ]
    }
  ]
}